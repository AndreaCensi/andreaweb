---
title: Research
page_class: research_index
Date: Sep 1 2007
---

> Ah, la recherche! Du temps perdu[.](http://projecteuclid.org/euclid.em/1062620828)
{:style="font-style:italic; text-align:center"}


###  My research interests



**Problems**

* My main interest lies in **robotics**, which, from my point of view, includes all 
  interactions of an intelligent agent with the real world. I believe that in my
  lifetime we will be able to create some form of [strong AI], and my hunch is
  that it will come from robotics. I hope to retire as a [robopsychologist].
  
* I am very fascinated 

  Currently 
  
interestection

* I'm interested in 

**my approach**

* Formal

* Measurement space

* Data driven Learning

* Reproducible research / open source & open data

**Methods and Techniques**



[Strong AI]: http://en.wikipedia.org/wiki/Strong_AI
[robopsychologist]: http://en.wikipedia.org/wiki/Robopsychology

Highlight #1 - Bootstrapping
=========================

My thesis work has to do with the problem of learning

The video below shows learning of this kind of models from recorded data, with
no prior information on the 

This is a 
My goal is to clearly define what can be learned from scratch

Representative papers:

* [Bootstrapping bilinear models of sensorimotor cascades][bds]

* Other preprints are undergoing double-blind reviews and cannot be published on a website; do not hesitate to ask for them.


[bds]: http://purl.org/censi/2010/boot


Video: Simulated visual input from tracking data of free-flying Drosophila Melanogaster ([watch on Vimeo][video_fly]. On the right, you can see the simulated visual stimulus. On the left, the position of the fly in the arena (1m diameter). Note that the video is slowed down about 10x with respect to the real data (you can see the real time in the left corner): fruit flies are very fast!  Watch the video full-screen to appreciate the details. 
{.caption}

<iframe src="http://player.vimeo.com/video/19194748" width="320" height="276" frameborder="0"></iframe>




Highlight #2 - Drosophila 
=========================

I have been collaborating with the Dickinson lab at Caltech (in the mean time moved
to University of Washington), in particular with [Andrew Straw][straw] (who now has his own lab atthe Institute of Molecular Pathology in Vienna, Austria) on the quantitative characterization of visually-driven behavior in Drosophila Melanogaster.

The <a href="http://vimeo.com/12689209">video</a> below shows the kind of simulated
data I am working with. Using [a special apparatus][mamarama] with 11 cameras, the animal is tracked with a precision of a few millimiters. An ad-hoc simulator allows to reconstruct the visual stimuls experienced by the animal at a reasonable level of accuracy.

We then analyze the data asking whether we can infer the neural processing happening in the animal brain.


Video: Simulated visual input from tracking data of free-flying Drosophila Melanogaster ([watch on Vimeo][video_fly]. On the right, you can see the simulated visual stimulus. On the left, the position of the fly in the arena (1m diameter). Note that the video is slowed down about 10x with respect to the real data (you can see the real time in the left corner): fruit flies are very fast!  Watch the video full-screen to appreciate the details. 
{.caption}

<iframe src="http://player.vimeo.com/video/19194748" width="320" height="276" frameborder="0"></iframe>

[video_fly]: http://vimeo.com/19194748

[straw]: http://strawlab.org
[mamarama]: http://www.its.caltech.edu/~astraw/research/flydra/



<!--

I'm mainly interested in **robotics**, and, in particular, in all things related to handling uncertainty (localization, estimation, planning, communication). I like simple algorithms with solid mathematical properties. I like the measurements space.

Recently, I have taken an interest in bio-plausible/bio-inspired control.


No, I don't have a thesis topic yet. Suggestions are welcome.
I try to keep only a handful of things in mind at a given time. At this very moment (Fall 2008), I'm focusing more on:
- Algebraic properties of planning in information spaces.
- Bayesian bounds for pose-tracking, SLAM; Gaussian processes.
- Value of information in multi-agent systems.
{:style="list-style-type: disc"}

Also, this term I will be taking courses in neuroscience / neuromorphic computing.
-->

Unorderd list of some topics
============================



See also:

- [Publications](../publications.html) 
- [Research software](sw/index.html)
{:style="list-style-type: disc"}

<!-- ![new](new.png){:new} -->


#### Control ####{:head}

* ![icon](../icons/bio-attitude.jpg){:icon}  How do flies navigate? Do flies learn to fly? <br/> ![new](new.png){:new}  [A bio-plausible design for visual attitude stabilization][bio-attitude]


#### Networks ####{:head}

* ![icon](../icons/fractals.jpg){:icon} The ultimate match: Cantor meets Kalman! <br/> ![new](new.png){:new}  [On the performance of Kalman filtering with intermittent observations: a geometric approach with fractals][fractals]

* ![icon](../icons/consensus.jpg){:icon}  How to reach a consensus if you and your friends are a bunch of spiking neurons. <br/> ![new](new.png){:new}  [Real-valued average consensus over noisy quantized channels][consensus]

  
#### Planning ####{:head}

* ![icon](../icons/ppu.jpg){:icon}   How to get to your goal without getting lost. <br/> [Robot motion planning with control and sensing uncertainty][ppu] 
  

#### Estimation ####{:head}


* ![icon](../icons/accuracy.jpg){:icon}  How precise can a localization method be? <br/> [The achievable accuracy for range-finder localization][aarl] ![new](new.png){:new} Journal Version

* ![icon](../icons/posetracking.jpg){:icon}   How precise can a scan-matching method be? <br/> ![new](new.png){:new} [On achievable accuracy for pose tracking][posetracking]

* ![icon](../icons/icpcov.jpg){:icon} How precise is ICP? <br/> [An accurate closed-form estimate of ICP's covariance][icpcov] 


#### Calibration ####{:head}

* ![icon](../icons/calibration.jpg){:icon}  The alternative to Borenstein if you have a range-finder. <br/> [Simultaneous maximum-likelihood calibration of odometry and sensor parameters][calibration]

#### Localization ####{:head}

* ![icon](../icons/ftf.jpg){:icon}  Global localization without the worries and anxiety of filtering. <br/>[Lazy Localization using the Frozen-Time Smoother][ftf]

* ![icon](../icons/ghtv.jpg){:icon} [A comparison of algorithms for likelihood approximation in Bayesian localization][ghtv] 

#### Scan matching ####{:head}

* ![icon](../icons/plicp.jpg){:icon} PLICP converges quadratically in a finite number of steps. <br/> [An ICP variant using a point-to-line metric][plicp] 

* ![icon](../icons/hsm.jpg){:icon} A global complete algorithm for scan matching. <br/> [Scan matching in the Hough domain][HSM]

* ![icon](../icons/hsm3d.jpg){:icon} HSM in 3D: much more difficult! <br/> ![new](new.png){:new}  [HSM3D: Feature-Less Global 6DOF Scan-Matching in the Hough/Radon Domain][hsm3d]

* ![icon](../icons/gpm.jpg){:icon} [GPM][GPM], which tries to make the most of the odometry model.



{:icon: style="display: block; float: left; clear: left; width: 64px; height: 64px; margin-right: 15px; margin-left: 15px; margin-top: 5px; margin-bottom: 15px; border: solid 1px black;"}

{:head: style="clear: both"}

{:new: style="margin-bottom: -10px"}


[fractals]: fractals.html
[consensus]: consensus.html
[csm]: sw/csm.html
[gpc]: gpc.html
[icpcov]: icpcov.html
[aarl]: tro-accuracy.html
[gpm]: gpm.html
[hsm]: hsm.html
[hsm3d]: hsm3d/index.html
[posetracking]: posetracking.html
[ghtv]: ghtv.html
[ppu]: ppu/index.html
[calibration]: calibration.html
[ftf]: ftf/index.html
[plicp]: plicp/index.html
[bio-attitude]: bio-attitude.html

