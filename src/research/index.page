---
title: Research interests
page_class: research_index
Date: Jan 1 2011
---


> Ah, la recherche! Du temps perdu.
> ([source](http://projecteuclid.org/euclid.em/1062620828))
{#quote}

This is a summary of my general research interests, 
along with some highlights of recent activity.

**General problem domains**

* My main interest lies in **robotics**, which, from my point of view, includes all 
  interactions of an intelligent agent with the real world. I believe that in my
  lifetime robotics will lead to some form of [strong AI], and I hope to retire as a [robopsychologist].
  
  **Perception** problems
  
  Inspiration: Asimov. I know, it's fiction, but I wouldn't be doing this without his books.
  
* I am fascinated by **cybernetics**, which, in the original meaning, of 
  neural information processing** in simple animals:
  nature's solutions are often much more robust than the current state
  of the art.
  
  Inspiration: [Cybernetics: Or the Control and Communication in the Animal and the Machine][wiener]

**Approaches and philosophy**

* In general, **simpler** is better than complicated. For robotics
  applications, this means staying close to the measurement space.
  
  Inspiration: Vehicles
  
* **Formal** is better than informal. Long-term lasting contributions
  are 
  
  Also, "formal" and "simple" are friends, because you usually can prove something
  only if it is simple enough.

  Inspiration: Dijkstra
    
* **Data** over models. driven Learning

  (Control community )
  Inspiration: Hinton's networks

* Reproducible research / open source & open data
  
  My generation is the first to 
   
  Inspiration: GNU/Linux.
  

**Favorite methods**

* Estimation and filtering

  What is possible, in principle?

* geometric objects 
 I have been lucky

  Inspiration: Shape spaces, and my favorite paper: [Covariance, subspace, and intrinsic Cramer-Rao bounds][smith] (which I still don't understand completely)
  
Keep reading for some highlights from my research, or read my [publications] list.
  
  
  
[Dijkstra]: http://www.cs.utexas.edu/users/EWD/
[kendall]: http://www.jstor.org/stable/2245331
[smith]: http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1420804
[wiener]: http://en.wikipedia.org/wiki/Norbert_Wiener
[Strong AI]: http://en.wikipedia.org/wiki/Strong_AI
[robopsychologist]: http://en.wikipedia.org/wiki/Robopsychology



Highlight - Bootstrapping Vehicles
=====================================

<div style='float: right'>
<img style='width: 320; height: 240'/>
</div>

My thesis work will focus on the problem of bootstrapping: learning a model of a robot's sensorimotor cascade, and its environment, from zero prior information other than basic semantic. At the beginning, we only assume to have uninterpreted streams of bits representing the output of some sensors, and that we have some commands available, and that there is some causal relation from actions to observations. The situation is very similar to the video on the right.

Other than a worthy problem per se (it subsumes most problems of learning, calibration, fault detection, etc.), it is also a proxy for studying some aspects of the higher level of neural processing. In fact, it is believed that [the cortex] starts as a *tabula rasa* that adapts to the inputs (the evidence is that parts of it can be repurposed in subjects that lost some capacity).

At this point, it is not clear if an agent can learn everything from the environment, or if there is something that should be known a priori. My goal has been to try to derive some precise formulation of the problem, and to find strong results (in the spirit of control theory) that can be built upon.

Representative papers:

* [Bootstrapping bilinear models of sensorimotor cascades][bds]

(other preprints are undergoing double-blind reviews and cannot be published on a website; do not hesitate to ask for them if you are interested)




Video: The video below shows learning of a bilinear model of a sensorimotor cascade for a camera. The agent starts with no previous knowledge on the sensor geometry, and by correlating observations with commands, it can learn a generative model for the data. The same model can be used for learning the dynamics of different sensors (range-finder, camera, field sampler).
See [many other videos][bevideos] of related experiments.
{.video_caption}

<!--<iframe src="http://player.vimeo.com/video/19194748" width="320" height="276" frameborder="0"></iframe>-->


[bds]: http://purl.org/censi/2010/boot

[many other videos]: bevideos


Highlight - Visually-guided behavior in Drosophila Melanogaster
================================================================

I have been collaborating with [the Dickinson lab], in particular with [Andrew Straw][straw] (who now has his own lab atthe Institute of Molecular Pathology in Vienna, Austria) on the quantitative characterization of visually-driven behavior in Drosophila Melanogaster.

The <a href="http://vimeo.com/12689209">video</a> below shows the kind of simulated
data I am working with. Using [a special apparatus][mamarama] with 11 cameras, the animal is tracked with a precision of a few millimiters. An ad-hoc simulator allows to reconstruct the visual stimuls experienced by the animal at a reasonable level of accuracy.

We then analyze the data asking whether we can infer the neural processing happening in the animal brain.


Video: Simulated visual input from tracking data of free-flying Drosophila Melanogaster ([watch on Vimeo][video_fly]. On the right, you can see the simulated visual stimulus. On the left, the position of the fly in the arena (1m diameter). Note that the video is slowed down about 10x with respect to the real data: fruit flies are very fast! (real time is in the left corner) Watch the video full-screen to appreciate the details. 
{.video_caption}

<!--<iframe src="http://player.vimeo.com/video/19194748" width="320" height="276" frameborder="0"></iframe>-->

[video_fly]: http://vimeo.com/19194748

[straw]: http://strawlab.org
[mamarama]: http://www.its.caltech.edu/~astraw/research/flydra/

[the Dickinson lab]: http://www.dickinson.caltech.edu/

<!--

I'm mainly interested in **robotics**, and, in particular, in all things related to handling uncertainty (localization, estimation, planning, communication). I like simple algorithms with solid mathematical properties. I like the measurements space.

Recently, I have taken an interest in bio-plausible/bio-inspired control.


No, I don't have a thesis topic yet. Suggestions are welcome.
I try to keep only a handful of things in mind at a given time. At this very moment (Fall 2008), I'm focusing more on:
- Algebraic properties of planning in information spaces.
- Bayesian bounds for pose-tracking, SLAM; Gaussian processes.
- Value of information in multi-agent systems.
{:style="list-style-type: disc"}

Also, this term I will be taking courses in neuroscience / neuromorphic computing.
-->

Unorderd list of some topics
============================



See also:

- [Publications](../publications.html) 
- [Research software](sw/index.html)
{:style="list-style-type: disc"}

[Publications]: ../publications.html

![new](new.png){:new}

#### Control ####{:head}

* ![icon](../icons/bio-attitude.jpg){:icon}  How do flies navigate? Do flies learn to fly? <br/>  [A bio-plausible design for visual attitude stabilization][bio-attitude]<br/> TOWRITE


#### Networks ####{:head}

* TOWRITE

* ![icon](../icons/fractals.jpg){:icon} The ultimate match: Cantor meets Kalman! <br/> [On the performance of Kalman filtering with intermittent observations: a geometric approach with fractals][fractals]

* ![icon](../icons/consensus.jpg){:icon}  How to reach a consensus if you and your friends are a bunch of spiking neurons. <br/>   [Real-valued average consensus over noisy quantized channels][consensus]

  
#### Planning ####{:head}

* ![icon](../icons/ppu.jpg){:icon}   How to get to your goal without getting lost. <br/> [Robot motion planning with control and sensing uncertainty][ppu] 
  

#### Estimation ####{:head}

* ![icon](../icons/accuracy.jpg){:icon}  How precise can a localization method be? <br/> [The achievable accuracy for range-finder localization][aarl] Journal Version

* ![icon](../icons/posetracking.jpg){:icon}   How precise can a scan-matching method be? <br/> [On achievable accuracy for pose tracking][posetracking]

* ![icon](../icons/icpcov.jpg){:icon} How precise is ICP? <br/> [An accurate closed-form estimate of ICP's covariance][icpcov] 


#### Calibration ####{:head}

* ![icon](../icons/calibration.jpg){:icon}  The alternative to Borenstein if you have a range-finder. <br/> [Simultaneous maximum-likelihood calibration of odometry and sensor parameters][calibration]

#### Localization and scan-matching ####{:head}

* ![icon](../icons/ftf.jpg){:icon}  Global localization without the worries and anxiety of filtering. <br/>[Lazy Localization using the Frozen-Time Smoother][ftf]

* ![icon](../icons/plicp.jpg){:icon} PLICP converges quadratically in a finite number of steps. <br/> [An ICP variant using a point-to-line metric][plicp] 

* ![icon](../icons/hsm.jpg){:icon} A global complete algorithm for scan matching. <br/> [Scan matching in the Hough domain][HSM]

* ![icon](../icons/hsm3d.jpg){:icon} HSM in 3D: much more difficult! <br/> [HSM3D: Feature-Less Global 6DOF Scan-Matching in the Hough/Radon Domain][hsm3d]

* ![icon](../icons/gpm.jpg){:icon} [GPM][GPM], which tries to make the most of the odometry model.

{:icon: style="display: block; float: left; clear: left; width: 64px; height: 64px; margin-right: 15px; margin-left: 15px; margin-top: 5px; margin-bottom: 15px; border: solid 1px black;"}

{:head: style="clear: both"}

{:new: style="margin-bottom: -10px"}


[fractals]: fractals.html
[consensus]: consensus.html
[csm]: sw/csm.html
[gpc]: gpc.html
[icpcov]: icpcov.html
[aarl]: tro-accuracy.html
[gpm]: gpm.html
[hsm]: hsm.html
[hsm3d]: hsm3d/index.html
[posetracking]: posetracking.html
[ghtv]: ghtv.html
[ppu]: ppu/index.html
[calibration]: calibration.html
[ftf]: ftf/index.html
[plicp]: plicp/index.html
[bio-attitude]: bio-attitude.html


<style type='text/css'>
    #quote { float: right; text-align: right; font-style:italic};
    .video_caption { font-style: italic; font-size: 95%}
</style>




