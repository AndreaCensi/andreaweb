[pub_ref_page id='censi13jbds_sub']

> **Abstract**: Learning and adaptivity will play a large role in robotics in the future, as robots transition from unstructured to unstructured environments that cannot be fully predicted or understood by the designer. Two questions that are open are how much it is possible to learn, and how much we should learn. The goal of bootstrapping is creating agents that are able to learn "everything" from scratch, including a torque-to-pixels models for its robotic body. Systems with such capabilities will be advantaged in terms of being resilient to unforeseen changes and deviations from prior assumptions. The robotics domain is a challenging one for learning, due to the presence of high-dimensional signals, various nonlinearities, and hidden states. There are no formal results, in the spirit of control theory, that could give guarantees. This paper considers the bootstrapping problem for a subset of the set of all robots. The Vehicles, inspired by Braitenberg's work, are idealization of mobile robots equipped with a set of “canonical” exteroceptive sensors (camera; range-finder; field-sampler). Their sensel-level dynamics are derived and shown to be surprising close. We define the class of BDS models, which assume an instantaneous bilinear dynamics between observations and commands, and derive streaming-based bilinear strategies for them. We show in what sense the BDS dynamics approximates the set of Vehicles to guarantee success in the task of generalized servoing: driving the observations to a given goal snapshot.  Simulations and experiments substantiate the theoretical results. This is the first instance of a bootstrapping agent that can learn the dynamics of a relatively large universe of systems, and use the models to solve well-defined tasks, with no parameter tuning or hand-designed features.

## Additional materials

- **slides**: 
  * [PDF format][slides-pdf] (14 MB)
  * [Keynote format, with cute videos][slides-key] (300 MB)
- **datasets and source code**: There is a  *reproducible research* Amazon EC2 image; [please see here for download instructions][code].

[code]: https://github.com/andreacensi/env_jbds/
[slides-pdf]: https://purl.org/censi/research/2013-jbds-slides.pdf
[slides-key]: https://purl.org/censi/research/2013-jbds-slides.key.zip


## Video with experimental data

<iframe src="//player.vimeo.com/video/80954603" width="1000" height="562" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>


<h2 style='margin-top: 5em'>Video snippets</h2>

These are some of the "video snippets" used in the [main video](https://player.vimeo.com/video/80954603) available for viewing and separate download.

### Exploration with motor babbling

<div class="flowplayer" data-ratio="0.5629">
   <video src="https://purl.org/censi/research/2013-jbds/videos/unicorn-babbling.mp4"></video>
</div>
<a href="https://purl.org/censi/research/2013-jbds/videos/unicorn-babbling.mp4">Download mp4</a>

### Tensor learning animations

<style>
TD.one { width: 20%; vertical-align: top; padding-top: 3em; padding-left: 1.4em;
padding-right: 1.4em;}
TD.one IMG { width: 100%;}
TD.two { width: 80%;}
</style>



#### One range-finder (YHL)

<table><tr>
    <td class='one'>
        <img src='https://purl.org/censi/research/2013-jbds/robot_icons/icon-unicornA_tw1_hl_sane_s4.pdf.png'/>
    </td>
    <td class='two'>
    <div class="flowplayer" data-ratio="0.5629">
       <video src="https://purl.org/censi/research/2013-jbds/videos/unicornA_tw1_hl_sane_s4-bdser_er4_i2_srl-alleps-video_bdse_learn_all_gs-spedup-chrome.mov.mp4"></video>
    </div></td>
    </tr>
</table>

<a href="https://purl.org/censi/research/2013-jbds/videos/unicornA_tw1_hl_sane_s4-bdser_er4_i2_srl-alleps-video_bdse_learn_all_gs-spedup-chrome.mov.mp4">Download mp4</a>


#### Two range-finders (YHLR)

<table><tr>
    <td class='one'>
        <img src='https://purl.org/censi/research/2013-jbds/robot_icons/icon-unicornA_tw1_hlhr_sane_s4.pdf.png'/>
    </td>
    <td class='two'>
    <div class="flowplayer" data-ratio="0.5629">
       <video src="https://purl.org/censi/research/2013-jbds/videos/unicornA_tw1_hlhr_sane_s4-bdser_er4_i2_srl-alleps-video_bdse_learn_all_gs-spedup-chrome.mov.mp4"></video>
    </div></td>
    </tr>
</table>

<a href="https://purl.org/censi/research/2013-jbds/videos/unicornA_tw1_hlhr_sane_s4-bdser_er4_i2_srl-alleps-video_bdse_learn_all_gs-spedup-chrome.mov.mp4">Download mp4</a>




#### 1D Camera (YVS)

<table><tr>
    <td class='one'>
        <img src='https://purl.org/censi/research/2013-jbds/robot_icons/icon-unicornA_tr1_cf_strip.pdf.png'/>
    </td>
    <td class='two'>
    <div class="flowplayer" data-ratio="0.5629">
       <video src="https://purl.org/censi/research/2013-jbds/videos/unicornA_tw1_cf_strip-bdser_e1_i2_ss-alleps-video_bdse_learn_all_gs-spedup-chrome.mov.mp4"></video>
    </div></td>
    </tr>
</table>


<a href="https://purl.org/censi/research/2013-jbds/videos/unicornA_tw1_cf_strip-bdser_e1_i2_ss-alleps-video_bdse_learn_all_gs-spedup-chrome.mov.mp4">Download mp4</a>

#### Field sampler (YFS)

<table><tr>
    <td class='one'>
        <img src='https://purl.org/censi/research/2013-jbds/robot_icons/icon-unicornA_tw1_fs1.pdf.png'/>
    </td>
    <td class='two'>
    <div class="flowplayer" data-ratio="0.5629">
       <video src="https://purl.org/censi/research/2013-jbds/videos/unicornA_tw1_fs1-bdse_e1_ss-alleps-video_bdse_learn_all_gs-spedup-chrome.mov.mp4"></video>
    </div></td>
    </tr>
</table>

<a href="https://purl.org/censi/research/2013-jbds/videos/unicornA_tw1_fs1-bdse_e1_ss-alleps-video_bdse_learn_all_gs-spedup-chrome.mov.mp4">Download mp4</a>






