[pub_ref_page id='censi11bgds']

## Videos


<ul>
   <li> <a href="http://purl.org/censi/research/2011-bgds/#sensels"> What the world looks like for a bootstrapping agent </a> </li>  <li> <a href="http://purl.org/censi/research/2011-bgds/#sick"> Range-finder data </a> </li>  <li> <a href="http://purl.org/censi/research/2011-bgds/#sick_stats"> Range-finder data - statistics </a> </li>  <li> <a href="http://purl.org/censi/research/2011-bgds/#sick_embed_popcode"> Range-finder data - embedding and population code </a> </li>  <li> <a href="http://purl.org/censi/research/2011-bgds/#sickpca_tensors"> Range-finder data - BGDS tensors learning </a> </li>  <li> <a href="http://purl.org/censi/research/2011-bgds/#rgb"> Camera data </a> </li>  <li> <a href="http://purl.org/censi/research/2011-bgds/#rgb_mean"> Camera data - mean </a> </li>  <li> <a href="http://purl.org/censi/research/2011-bgds/#rgb_var"> Camera data - variance </a> </li>  <li> <a href="http://purl.org/censi/research/2011-bgds/#grayscale_tensors"> Camera data - gray-scale - BGDS tensors learning </a> </li>  <li> <a href="http://purl.org/censi/research/2011-bgds/#contrast_tensors"> Camera data - contrast - BGDS tensors learning </a> </li>  <li> <a href="http://purl.org/censi/research/2011-bgds/#grayscale_ydot_ypred"> Camera data - grayscale - observations and predictions </a> </li>  <li> <a href="http://purl.org/censi/research/2011-bgds/#grayscale_detect"> Camera data - grayscale - anomaly detection signal </a> </li> 
</ul>

[videos]: http://purl.org/censi/research/2011-bgds/


<!-- 
<script type="text/javascript" src="flowplayer-3.2.4.min.js"></script> 
<style type='text/css'>
div.video {
    clear: both;
    padding: 1em;
}
.video h2 {
    padding-top: 2em;
    border-bottom: solid 2px black;
}
.video .others {
margin-left: 1em;
}

.video .generated { clear: both; }

.video .widget {
/*    float:left; */
  display:block; width:400px; height:300px;
    background-color: #aad;
    margin: 1em;
}

.widget_container {
  float: right;
}

h3 { display: inline }

A.widget { 
  text-align: right;
  font-family: monospace;
  font-size: 20px;
  color: #f66;
}
.video A.play {
    font-weight: bold;
    color: red;
    margin-right: 2em;
    margin-left: 2em;
    text-decoration: none !important;
}

body  P { max-width: 35em }
</style>

<p><strong>Data</strong>: The original data is from the <a href="http://www.rawseeds.org/">Rawseeds</a> project.</p>
<p><strong>Videos format</strong>: The videos are in <a href="http://en.wikipedia.org/w/index.php?title=MPEG-4_Part_14&amp;oldid=421498413">MP4</a> format with <a href="http://en.wikipedia.org/w/index.php?title=H.264/MPEG-4_AVC&amp;oldid=420253210">H264</a> encoding. They were encoded as .avi/mpeg using mencoder, then converted to .mp4/h264 with ffmpeg. They should play on any recent/decent player, so let us know if it doesn't work for you. Free players that are known to work include: <a href="http://www.mplayerhq.hu/">MPlayer</a>, <a href="http://www.videolan.org/vlc/">VLC</a>.</p>
<p>Click "play" to play the video in the browser using a Flash widget. Or right-click "download" for the direct link to the .mp4 file.</p>
<p><a style='float:right' href="http://creativecommons.org/licenses/by-sa/3.0/">
    <img src="http://i.creativecommons.org/l/by-sa/3.0/88x31.png"/>
</a></p>
<p><strong>License</strong>: You are welcome to use these videos under the terms of the <a href="http://creativecommons.org/licenses/by-sa/3.0/">Creative Commons Attribution-ShareAlike 3.0 Unported License</a>.</p>
      
<div id="indexdiv">
<h3 id="index">Index</h3>
<ul>
   <li> <a href="http://purl.org/censi/research/2011-bgds/#sensels"> What the world looks like for a bootstrapping agent </a> </li>  <li> <a href="http://purl.org/censi/research/2011-bgds/#sick"> Range-finder data </a> </li>  <li> <a href="http://purl.org/censi/research/2011-bgds/#sick_stats"> Range-finder data - statistics </a> </li>  <li> <a href="http://purl.org/censi/research/2011-bgds/#sick_embed_popcode"> Range-finder data - embedding and population code </a> </li>  <li> <a href="http://purl.org/censi/research/2011-bgds/#sickpca_tensors"> Range-finder data - BGDS tensors learning </a> </li>  <li> <a href="http://purl.org/censi/research/2011-bgds/#rgb"> Camera data </a> </li>  <li> <a href="http://purl.org/censi/research/2011-bgds/#rgb_mean"> Camera data - mean </a> </li>  <li> <a href="http://purl.org/censi/research/2011-bgds/#rgb_var"> Camera data - variance </a> </li>  <li> <a href="http://purl.org/censi/research/2011-bgds/#grayscale_tensors"> Camera data - gray-scale - BGDS tensors learning </a> </li>  <li> <a href="http://purl.org/censi/research/2011-bgds/#contrast_tensors"> Camera data - contrast - BGDS tensors learning </a> </li>  <li> <a href="http://purl.org/censi/research/2011-bgds/#grayscale_ydot_ypred"> Camera data - grayscale - observations and predictions </a> </li>  <li> <a href="http://purl.org/censi/research/2011-bgds/#grayscale_detect"> Camera data - grayscale - anomaly detection signal </a> </li> 
</ul>
</div>
  
 
<div class="video"> <h2 id="sensels"> What the world looks like for a bootstrapping agent </h2>

    <div class="widget_container">
    
        <a class='play' href="http://purl.org/censi/research/2011-bgds/#sensels"
          onclick='flowplayer("senselswidget", "flowplayer-3.2.4.swf", {clip: { scaling: "fit", url:"http://purl.org/censi/research/2011-bgds/videos/sensels.mp4"} });'> 
          play 
        </a>  
    
        <a class="download" href="http://purl.org/censi/research/2011-bgds/videos/sensels.mp4"> download (2.1 mp4) </a>

        <a  class="widget" style="background-image: url('http://purl.org/censi/research/2011-bgds/videos/sensels.mp4.png')"  href="http://purl.org/censi/research/2011-bgds/#sensels"
          onclick='flowplayer("senselswidget", "flowplayer-3.2.4.swf", {clip: { scaling: "fit", url:"http://purl.org/censi/research/2011-bgds/videos/sensels.mp4"} });'
            id = "senselswidget" >  </a>

    
    </div>

<p>This animation shows the initial state of knowledge for a bootstrapping agent.
At the beginning, we only have an uninterpreted stream of observations and commands,
and we must make sense of them. The bootstrapping agent must interpret this initial confusion. The only
semantics assumed is that the commands somehow have a causal effect on the observations.
Can you tell which sensor is this?   <br />
</p>
<p>At left, you see the observations, which in the paper are called <code>y</code> (white: low, black: high, whathever "high" and "low" mean for the unknown sensors); in the middle, the derivative  <code>dy/dt</code> of the observations (red: positive, white: zero, blue: negative); at right, the uninterpreted commands <code>u</code>.
In this case, the commands correspond to linear and angular velocities.</p>
<p>Can you tell which changes in the observations are due to the agent actions (motion) or other things moving in the environment? This is the <strong>anomaly detection</strong> task considered in the paper.
This is a <em>passive</em> task that can be done on logged data, compared to the servoing task,
considered in our previous work, which being <em>active</em> is more representative of the model learned, but cannot be evaluated on static data.</p>
        

</div>

<div class="video"> <h2 id="sick"> Range-finder data </h2>

        <div class="widget_container">
        
            <a class='play' href="http://purl.org/censi/research/2011-bgds/#sick"
              onclick='flowplayer("sickwidget", "flowplayer-3.2.4.swf", {clip: { scaling: "fit", url:"http://purl.org/censi/research/2011-bgds/videos/sick.mp4"} });'> 
              play 
            </a>  
        
            <a class="download" href="http://purl.org/censi/research/2011-bgds/videos/sick.mp4"> download (92.4 mp4) </a>

            <a  class="widget" style="background-image: url('http://purl.org/censi/research/2011-bgds/videos/sick.mp4.png')"  href="http://purl.org/censi/research/2011-bgds/#sick"
              onclick='flowplayer("sickwidget", "flowplayer-3.2.4.swf", {clip: { scaling: "fit", url:"http://purl.org/censi/research/2011-bgds/videos/sick.mp4"} });'
                id = "sickwidget" >  </a>

        
        </div>

        <p>This video displays the laser data obtained by the two Sick range-finders mounted at
approximately 0deg and 180deg with respect to the robot front.</p>
<p>On the right, the raw readings are displayed by a simple plot. 
The first 181 readings are from the front laser, and the rest from the back laser.</p>
<p>On the left, the laser readings are plotted in polar form superimposed to the data 
from the omnidirectional camera. (Note that the alignment is only approximative).</p>
<p>In both cases the maximum distance is capped at 20m (for visualization purposes).</p>
        

</div>

<div class="video"> <h2 id="sick_stats"> Range-finder data - statistics </h2>

        <div class="widget_container">
        
            <a class='play' href="http://purl.org/censi/research/2011-bgds/#sick_stats"
              onclick='flowplayer("sick_statswidget", "flowplayer-3.2.4.swf", {clip: { scaling: "fit", url:"http://purl.org/censi/research/2011-bgds/videos/sick_stats.mp4"} });'> 
              play 
            </a>  
        
            <a class="download" href="http://purl.org/censi/research/2011-bgds/videos/sick_stats.mp4"> download (6.0 mp4) </a>

            <a  class="widget" style="background-image: url('http://purl.org/censi/research/2011-bgds/videos/sick_stats.mp4.png')"  href="http://purl.org/censi/research/2011-bgds/#sick_stats"
              onclick='flowplayer("sick_statswidget", "flowplayer-3.2.4.swf", {clip: { scaling: "fit", url:"http://purl.org/censi/research/2011-bgds/videos/sick_stats.mp4"} });'
                id = "sick_statswidget" >  </a>

        
        </div>

        <p>This video displays some second-order statistics of the laser data, namely the sample covariance of the readings (left), of the readings derivative (center), and of the sign of the readings derivative (right).</p>
<p>Eventually, when averaged over long trajectories in various environments, all of the three statistics are a function of the distance between the sensels. However, their convergence properties vary. The covariance of the readings is slow to converge because the robot is driven along stereotypical trajectories (e.g., straight in the middle of a corridor). Instead, the derivative of the readings converges faster (informally, deriving something tends to get rid of the slow phenomena). </p>
<p>From second-order statistics, it is possible to infer <em>similarities</em> of the sensels, and from those similarities, to obtain an <strong>embedding</strong> of the sensels on the sensel space. The <em>metric</em> information is not recovered precisely, but the <em>topology</em> can be reliably estimated. In this case, this means that, even starting from shuffled values, it is possible to recover the ordering of the sensels. The remaining uncertainty can be considered a <strong>diffeomorphism nuisance</strong>.</p>
<p>Actually, the most reliable statistics for embedding purposes is the <em>information distance</em> between the sensels, which is not shown here. Here is <a href="sick.pickle">a Python pickle file containing the information distance matrix</a> estimated from the data.  <br />
</p>
        

</div>

<div class="video"> <h2 id="sick_embed_popcode"> Range-finder data - embedding and population code </h2>

        <div class="widget_container">
        
            <a class='play' href="http://purl.org/censi/research/2011-bgds/#sick_embed_popcode"
              onclick='flowplayer("sick_embed_popcodewidget", "flowplayer-3.2.4.swf", {clip: { scaling: "fit", url:"http://purl.org/censi/research/2011-bgds/videos/sick_embed_popcode.mp4"} });'> 
              play 
            </a>  
        
            <a class="download" href="http://purl.org/censi/research/2011-bgds/videos/sick_embed_popcode.mp4"> download (27.7 mp4) </a>

            <a  class="widget" style="background-image: url('http://purl.org/censi/research/2011-bgds/videos/sick_embed_popcode.mp4.png')"  href="http://purl.org/censi/research/2011-bgds/#sick_embed_popcode"
              onclick='flowplayer("sick_embed_popcodewidget", "flowplayer-3.2.4.swf", {clip: { scaling: "fit", url:"http://purl.org/censi/research/2011-bgds/videos/sick_embed_popcode.mp4"} });'
                id = "sick_embed_popcodewidget" >  </a>

        
        </div>

        <p>Using statistics of the data we can recover the ordering of sensels.
The next step is <strong>percentile normalization</strong>: the value of each sensel is normalized in the [0,100] range according to its percentile in the sequence. This step normalizes a diffeomorphism nuisance acting on the values. For example, suppose that the data of a range-finder are  modified by an invertible nonlinearity, such as <code>x --&gt; 1/x</code>, so that the values represent nearness instead of distances. The percentile normalization step equalizes the effect of such nonlinearity.</p>
<p>Also, it has the effect that the data is represented more densely for more probable values, which makes it an efficient representation. For example, the range-finder readings are mostly in the &lt;10m range, while very few samples are &gt;20m. The percentile representation gives more space to the immediate surroundings.</p>
<p>The next step is the <strong>population code</strong> computation. N cells are assigned to each sensel. Each cell is activated if the sensel value is close to its reference point, according to a certain kernel.
The result is a 362xN array which we can display as an image for easy visualization.</p>
<p>Now, notice that all these operations are generic and data-agnostic. However, for the case of the range-finder, the end result is an image which is diffeomorphic to a polar map of the environment. On the <em>y</em> axis we have the sensel position in the sensel space, which is the angle (up to a diffeomorphism); on the <em>y</em> axis we have the percentile as a pop. code, which is diffeomorphic to the range.</p>
<p>In the paper we prove that the range-finder data thus preprocessed can be approximated by a BGDS model.</p>
        

     </div>

    <div class="video"> <h2 id="sickpca_tensors"> Range-finder data - BGDS tensors learning </h2>

<div class="widget_container">

    <a class='play' href="http://purl.org/censi/research/2011-bgds/#sickpca_tensors"
      onclick='flowplayer("sickpca_tensorswidget", "flowplayer-3.2.4.swf", {clip: { scaling: "fit", url:"http://purl.org/censi/research/2011-bgds/videos/sickpca_tensors.mp4"} });'> 
      play 
    </a>  

    <a class="download" href="http://purl.org/censi/research/2011-bgds/videos/sickpca_tensors.mp4"> download (2.5 mp4) </a>

    <a  class="widget" style="background-image: url('http://purl.org/censi/research/2011-bgds/videos/sickpca_tensors.mp4.png')"  href="http://purl.org/censi/research/2011-bgds/#sickpca_tensors"
      onclick='flowplayer("sickpca_tensorswidget", "flowplayer-3.2.4.swf", {clip: { scaling: "fit", url:"http://purl.org/censi/research/2011-bgds/videos/sickpca_tensors.mp4"} });'
        id = "sickpca_tensorswidget" >  </a>


</div>

<p>This video shows the tensor H being learned for the range-finder data. 
Its four slices are shown side-by-side in false colors (red: positive, white: zero, blue: negative). The video is only for 1 log out of 11, so the final
results are not as smooth as those shown in the paper's figures.</p>
  

</div>

<div class="video"> <h2 id="rgb"> Camera data </h2>

  <div class="widget_container">
  
      <a class='play' href="http://purl.org/censi/research/2011-bgds/#rgb"
        onclick='flowplayer("rgbwidget", "flowplayer-3.2.4.swf", {clip: { scaling: "fit", url:"http://purl.org/censi/research/2011-bgds/videos/rgb.mp4"} });'> 
        play 
      </a>  
  
      <a class="download" href="http://purl.org/censi/research/2011-bgds/videos/rgb.mp4"> download (240.7 mp4) </a>

      <a  class="widget" style="background-image: url('http://purl.org/censi/research/2011-bgds/videos/rgb.mp4.png')"  href="http://purl.org/censi/research/2011-bgds/#rgb"
        onclick='flowplayer("rgbwidget", "flowplayer-3.2.4.swf", {clip: { scaling: "fit", url:"http://purl.org/censi/research/2011-bgds/videos/rgb.mp4"} });'
          id = "rgbwidget" >  </a>

  
  </div>

  <p>This is the composite frame used as the data in the experiments.</p>
<p>On the left, the data from the omnidirectional camera. Top right is a frontal camera
with large field of view, and bottom right one of the Three triclops cameras.</p>
<p>Unfortunately, there might be some glitches due to the fact that different cameras
have different framerates.</p>
  

</div>

<div class="video"> <h2 id="rgb_mean"> Camera data - mean </h2>

  <div class="widget_container">
  
      <a class='play' href="http://purl.org/censi/research/2011-bgds/#rgb_mean"
        onclick='flowplayer("rgb_meanwidget", "flowplayer-3.2.4.swf", {clip: { scaling: "fit", url:"http://purl.org/censi/research/2011-bgds/videos/rgb_mean.mp4"} });'> 
        play 
      </a>  
  
      <a class="download" href="http://purl.org/censi/research/2011-bgds/videos/rgb_mean.mp4"> download (0.9 mp4) </a>

      <a  class="widget" style="background-image: url('http://purl.org/censi/research/2011-bgds/videos/rgb_mean.mp4.png')"  href="http://purl.org/censi/research/2011-bgds/#rgb_mean"
        onclick='flowplayer("rgb_meanwidget", "flowplayer-3.2.4.swf", {clip: { scaling: "fit", url:"http://purl.org/censi/research/2011-bgds/videos/rgb_mean.mp4"} });'
          id = "rgb_meanwidget" >  </a>

  
  </div>

  <p>This video shows the computation of the mean values for each pixel. Notice 
how simple statistics identify the role</p>
  

</div>

<div class="video"> <h2 id="rgb_var"> Camera data - variance </h2>

  <div class="widget_container">
  
      <a class='play' href="http://purl.org/censi/research/2011-bgds/#rgb_var"
        onclick='flowplayer("rgb_varwidget", "flowplayer-3.2.4.swf", {clip: { scaling: "fit", url:"http://purl.org/censi/research/2011-bgds/videos/rgb_var.mp4"} });'> 
        play 
      </a>  
  
      <a class="download" href="http://purl.org/censi/research/2011-bgds/videos/rgb_var.mp4"> download (0.9 mp4) </a>

      <a  class="widget" style="background-image: url('http://purl.org/censi/research/2011-bgds/videos/rgb_var.mp4.png')"  href="http://purl.org/censi/research/2011-bgds/#rgb_var"
        onclick='flowplayer("rgb_varwidget", "flowplayer-3.2.4.swf", {clip: { scaling: "fit", url:"http://purl.org/censi/research/2011-bgds/videos/rgb_var.mp4"} });'
          id = "rgb_varwidget" >  </a>

  
  </div>

  <p>This video shows the computation of the variance for each pixel. Here white = low variance,
and dark = high variance. In general, it is not necessarily true that high variance is equivalent
to more information, but in this case extremely low-variance pixels are non informative
and could be discarded. Those correspond to "dead" parts of the image: borders or the robot
itself reflected in the images.</p>
  

</div>

<div class="video"> <h2 id="grayscale_tensors"> Camera data - gray-scale - BGDS tensors learning </h2>

  <div class="widget_container">
  
      <a class='play' href="http://purl.org/censi/research/2011-bgds/#grayscale_tensors"
        onclick='flowplayer("grayscale_tensorswidget", "flowplayer-3.2.4.swf", {clip: { scaling: "fit", url:"http://purl.org/censi/research/2011-bgds/videos/grayscale_tensors.mp4"} });'> 
        play 
      </a>  
  
      <a class="download" href="http://purl.org/censi/research/2011-bgds/videos/grayscale_tensors.mp4"> download (2.9 mp4) </a>

      <a  class="widget" style="background-image: url('http://purl.org/censi/research/2011-bgds/videos/grayscale_tensors.mp4.png')"  href="http://purl.org/censi/research/2011-bgds/#grayscale_tensors"
        onclick='flowplayer("grayscale_tensorswidget", "flowplayer-3.2.4.swf", {clip: { scaling: "fit", url:"http://purl.org/censi/research/2011-bgds/videos/grayscale_tensors.mp4"} });'
          id = "grayscale_tensorswidget" >  </a>

  
  </div>

  <p>This video shows the tensor H being learned for the camera data. 
Its four slices are shown side-by-side in false colors (red: positive, white: zero, blue: negative). </p>
<p>The video is only for 1 log out of 11, so the final
results are not as smooth as those shown in the paper's figures.</p>
<p>In this video the false-color image of each tensor slice 
is generated independently from the other slices. This visualization exaggerates the
the importance of the (vertical gradient, angular velocity, at the bottom right) 
slide, which is 0 theoretically and just noise in practice, and would appear white
if all slices are normalized together as in the paper's figures.</p>
  

</div>

<div class="video"> <h2 id="contrast_tensors"> Camera data - contrast - BGDS tensors learning </h2>

  <div class="widget_container">
  
      <a class='play' href="http://purl.org/censi/research/2011-bgds/#contrast_tensors"
        onclick='flowplayer("contrast_tensorswidget", "flowplayer-3.2.4.swf", {clip: { scaling: "fit", url:"http://purl.org/censi/research/2011-bgds/videos/contrast_tensors.mp4"} });'> 
        play 
      </a>  
  
      <a class="download" href="http://purl.org/censi/research/2011-bgds/videos/contrast_tensors.mp4"> download (2.5 mp4) </a>

      <a  class="widget" style="background-image: url('http://purl.org/censi/research/2011-bgds/videos/contrast_tensors.mp4.png')"  href="http://purl.org/censi/research/2011-bgds/#contrast_tensors"
        onclick='flowplayer("contrast_tensorswidget", "flowplayer-3.2.4.swf", {clip: { scaling: "fit", url:"http://purl.org/censi/research/2011-bgds/videos/contrast_tensors.mp4"} });'
          id = "contrast_tensorswidget" >  </a>

  
  </div>

  <p>This is the same as the previous video, with the difference that we are pre-filtering
the camera data using a contrast operation before learning. The results are very similar.
In general, filtering the data with a local operation should not change the learning result.</p>
  

</div>

<div class="video"> <h2 id="grayscale_ydot_ypred"> Camera data - grayscale - observations and predictions </h2>

<div class="widget_container">

    <a class='play' href="http://purl.org/censi/research/2011-bgds/#grayscale_ydot_ypred"
      onclick='flowplayer("grayscale_ydot_ypredwidget", "flowplayer-3.2.4.swf", {clip: { scaling: "fit", url:"http://purl.org/censi/research/2011-bgds/videos/grayscale_ydot_ypred.mp4"} });'> 
      play 
    </a>  

    <a class="download" href="http://purl.org/censi/research/2011-bgds/videos/grayscale_ydot_ypred.mp4"> download (314.8 mp4) </a>

    <a  class="widget" style="background-image: url('http://purl.org/censi/research/2011-bgds/videos/grayscale_ydot_ypred.mp4.png')"  href="http://purl.org/censi/research/2011-bgds/#grayscale_ydot_ypred"
      onclick='flowplayer("grayscale_ydot_ypredwidget", "flowplayer-3.2.4.swf", {clip: { scaling: "fit", url:"http://purl.org/censi/research/2011-bgds/videos/grayscale_ydot_ypred.mp4"} });'
        id = "grayscale_ydot_ypredwidget" >  </a>


  </div>

  <p>This video shows side-by-side the derivative of the data and the prediction based on the 
  learning tensors.</p>
  <p>Note that fast rotations cause problems because of motion blur and the fact that
  the time resolution becomes relevant, so that the observations cannot be explained by
  a continuous dynamics.
  Here we should take a multi-scale approach, reducing the resolution of the image
  for fast motions.</p>
</div>
<p> ciao</p>
<div class="video"> 
  <h2 id="grayscale_detect"> Camera data - grayscale - anomaly detection signal </h2>

  <div class="widget_container">
  
      <a class='play' href="http://purl.org/censi/research/2011-bgds/#grayscale_detect"
        onclick='flowplayer("grayscale_detectwidget", "flowplayer-3.2.4.swf", {clip: { scaling: "fit", url:"http://purl.org/censi/research/2011-bgds/videos/grayscale_detect.mp4"} });'> 
        play 
      </a>  
  
      <a class="download" href="http://purl.org/censi/research/2011-bgds/videos/grayscale_detect.mp4"> download (85.5 mp4) </a>

      <a  class="widget" style="background-image: url('http://purl.org/censi/research/2011-bgds/videos/grayscale_detect.mp4.png')"  href="http://purl.org/censi/research/2011-bgds/#grayscale_detect"
        onclick='flowplayer("grayscale_detectwidget", "flowplayer-3.2.4.swf", {clip: { scaling: "fit", url:"http://purl.org/censi/research/2011-bgds/videos/grayscale_detect.mp4"} });'
          id = "grayscale_detectwidget">  </a>

  
  </div>

<p>This video shows the anomaly detection signal (white: no anomaly, black: anomaly).</p>
<p>The false colors are normalized per-frame. This means that
in the first part of the sequence, when there are no moving objects,
you are looking mainly at noise.</p>
<p>Skip to time 45s or 105s (in log time, displayed in the bottom right) to see the strong 
signal when people walk past the robot.</p>
<p>Also, genuine anomalies that do not correspond to moving objects but to model failures are:</p>
<ul>
<li>occlusions</li>
<li>motion blur due to large rotations</li>
<li>when the robot tilts due to uneven pavement, which
  produces a motion not explained by linear/angular velocities</li>
</ul> -->